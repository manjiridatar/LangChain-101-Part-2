{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd27b67c",
   "metadata": {},
   "source": [
    "# LangChain - Quickstart Guide - The Basics - Part 2 (ChatModel)\n",
    "\n",
    "### LangChain is a framework which simplifies the development of apps using LLMs like OpenAI's GPT-4\n",
    "\n",
    "##### Quickstart Guide location = https://python.langchain.com/en/latest/getting_started/getting_started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af654da",
   "metadata": {},
   "source": [
    "### Install LangChain and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c66eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80d1fc",
   "metadata": {},
   "source": [
    "#### Lets Import OS and Set the API Keys\n",
    "\n",
    "We need to set the environment variable \"OPENAI_API_KEY\" and SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda34e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf1f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3dadb4",
   "metadata": {},
   "source": [
    "#### Import the ChatOpenAI wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8796d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636bf72",
   "metadata": {},
   "source": [
    "### Chat Models:  Use LLM under the hood. \n",
    "\n",
    "The interface is different from what we saw with LLM. Instead of text in text out Chat Models are message in message out as inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24a8c2",
   "metadata": {},
   "source": [
    "#### Let's also import the schemas for: AIMessage, HumanMessage and  SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbaf7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7e9f1c",
   "metadata": {},
   "source": [
    "#### Create the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2131cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb47ea7",
   "metadata": {},
   "source": [
    "#### Get message completion from the chat model by passing in a single message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a8473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"C'est une belle journée !\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content = \"Translate this sentence from English to French. It is a beautiful day!\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99c774",
   "metadata": {},
   "source": [
    "#### Pass in Multiple messages for OpenAI’s GPT-3.5-turbo and GPT-4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97e879f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"C'est une belle journée !\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature = 0, model_name = 'gpt-3.5-turbo')\n",
    "messages = [\n",
    "    SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
    "    HumanMessage(content=\"It is a beautiful day!\")\n",
    "]\n",
    "\n",
    "chat(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3b80a",
   "metadata": {},
   "source": [
    "#### Construct multiple completions by passing multiple sets of messages and calling 'generate' on the chat to return LLMResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8e7867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"C'est une belle journée !\", generation_info=None, message=AIMessage(content=\"C'est une belle journée !\", additional_kwargs={}, example=False))], [ChatGeneration(text='Allons faire une randonnée !', generation_info=None, message=AIMessage(content='Allons faire une randonnée !', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 59, 'completion_tokens': 14, 'total_tokens': 73}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
    "        HumanMessage(content = \"It is a beautiful day!\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
    "        HumanMessage(content = \"Let's go for a hike!\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_messages)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5969e",
   "metadata": {},
   "source": [
    "#### LLMResult can be queried for things like token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fc86acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 59, 'completion_tokens': 14, 'total_tokens': 73}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281db8ba",
   "metadata": {},
   "source": [
    "### ChatPromtTemplates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed46db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Il fait beau aujourd'hui !\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the templates\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# create the chat\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0)\n",
    "\n",
    "# set the system message prompt template\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# set the human message prompt template\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# creaet the chat prompt template\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# format the prompt using the chat template and input for the templates. Then generate out put.\n",
    "\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", \n",
    "                               output_language = \"French\", \n",
    "                               text = \"It is a beautiful day!\").to_messages())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b68c1e",
   "metadata": {},
   "source": [
    "### Chains - Combine Chat Models and ChatPromptTemplates into a multi-step workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d61daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    )\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0)\n",
    "\n",
    "# set the system message prompt template\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# set the human message prompt template\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Set the chat prompt template using the system and human prompt template\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b2ddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a3f08db4008a99764362e29e57db00fd in your message.).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Il fait beau aujourd'hui !\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and run the chain using the chat prompt template we just set\n",
    "\n",
    "chain = LLMChain(llm = chat, prompt = chat_prompt)\n",
    "chain.run(input_language = \"English\", output_language = \"French\", text = \"It is a beautiful day!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e548b0",
   "metadata": {},
   "source": [
    "### Agents - Dynamically call Chains based on user inputs\n",
    "\n",
    "Use agents to determine which action to take and in what order. The use is similar for chat models to the llm.\n",
    "\n",
    "A tool is a function that performs a specified function. This can be things like: Google Search, Database lookup, Python REPL, other chains. \n",
    "\n",
    "Agents: For a list of supported agents and their specifications, https://python.langchain.com/en/latest/modules/agents/agents.html\n",
    "\n",
    "Tools: For a list of predefined tools and their specifications, https://python.langchain.com/en/latest/modules/agents/tools.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02af92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: How tall is Niagara Falls? What would be its height if it was two times as tall?\n",
      "Thought: I need to use a search engine to find the height of Niagara Falls and then use a calculator to double that height.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Niagara Falls height\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m325′\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to use a calculator to double the height of Niagara Falls.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"325*2\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 650\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: Niagara Falls is 325 feet tall. If it was two times as tall, it would be 650 feet tall.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Niagara Falls is 325 feet tall. If it was two times as tall, it would be 650 feet tall.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# Let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Now let's load tools to use we plan to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the chat model, and the type of agent we want to use.\n",
    "\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"How tall is Niagara Falls? What would be its height if it was two times as tall?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405aa3d",
   "metadata": {},
   "source": [
    "### Memory - Add State to Chains and Agents\n",
    "\n",
    "If our application requires our chains and agent to have memory of the previous conversation we use ConversationChain.\n",
    "The difference between using ConversationChain with llm vs chat)_modl is that, the messages can be kept in their own memory object instead of being condensed to a string. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "564a1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "            MessagesPlaceholder(variable_name = \"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e76724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the chat and conversation chain\n",
    "\n",
    "chat = ChatOpenAI(temperature = 0)\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "conversation = ConversationChain(memory = memory, prompt = prompt, llm = chat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "421e27ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI language model, so I don't have feelings like humans do, but I'm functioning properly and ready to assist you. How can I help you today?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the next response\n",
    "\n",
    "conversation.predict(input=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "999768de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, the Catskills are a great place for hiking! There are many trails to choose from, ranging from easy to difficult. Some popular hikes in the Catskills include Kaaterskill Falls, Overlook Mountain, and Devil's Path. Kaaterskill Falls is a two-stage waterfall that is one of the most popular hikes in the area. Overlook Mountain is a moderate hike that offers stunning views of the Hudson Valley. Devil's Path is a challenging hike that is known for its steep climbs and rocky terrain.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue the conversation to the next predict\n",
    "\n",
    "conversation.predict(input=\"It is a beautiful day! Going out for a hike would be nice. Good hikes in the Catskills?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4452843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! As I mentioned earlier, I'm an AI language model designed to assist with various tasks such as answering questions, generating text, and providing information. I was created using advanced machine learning algorithms and trained on a large dataset of text. My goal is to provide accurate and helpful responses to your queries. I don't have personal preferences or emotions like humans do, but I'm always here to help you with any questions or tasks you may have.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue the conversation to the next predict\n",
    "\n",
    "conversation.predict(input=\"Thanks! Tell me about yourself \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3c2b4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hello, how are you?', additional_kwargs={}, example=False), AIMessage(content=\"Hello! I'm an AI language model, so I don't have feelings like humans do, but I'm functioning properly and ready to assist you. How can I help you today?\", additional_kwargs={}, example=False), HumanMessage(content='It is a beautiful day! Going out for a hike would be nice. Good hikes in the Catskills?', additional_kwargs={}, example=False), AIMessage(content=\"Yes, the Catskills are a great place for hiking! There are many trails to choose from, ranging from easy to difficult. Some popular hikes in the Catskills include Kaaterskill Falls, Overlook Mountain, and Devil's Path. Kaaterskill Falls is a two-stage waterfall that is one of the most popular hikes in the area. Overlook Mountain is a moderate hike that offers stunning views of the Hudson Valley. Devil's Path is a challenging hike that is known for its steep climbs and rocky terrain.\", additional_kwargs={}, example=False), HumanMessage(content='It is a beautiful day! Going out for a hike would be nice. Good hikes in the Catskills?', additional_kwargs={}, example=False), AIMessage(content=\"Yes, the Catskills are a great place for hiking! There are many trails to choose from, ranging from easy to difficult. Some popular hikes in the Catskills include Kaaterskill Falls, Overlook Mountain, and Devil's Path. Kaaterskill Falls is a two-stage waterfall that is one of the most popular hikes in the area. Overlook Mountain is a moderate hike that offers stunning views of the Hudson Valley. Devil's Path is a challenging hike that is known for its steep climbs and rocky terrain.\", additional_kwargs={}, example=False), HumanMessage(content='Thanks! Tell me about yourself ', additional_kwargs={}, example=False), AIMessage(content=\"Sure! As I mentioned earlier, I'm an AI language model designed to assist with various tasks such as answering questions, generating text, and providing information. I was created using advanced machine learning algorithms and trained on a large dataset of text. My goal is to provide accurate and helpful responses to your queries. I don't have personal preferences or emotions like humans do, but I'm always here to help you with any questions or tasks you may have.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='history')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4388765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
      "os.environ[\"SERPAPI_API_KEY\"] = \"\"\n",
      "#from langchain.llms import OpenAI\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema import (\n",
      "    AIMessage,\n",
      "    HumanMessage,\n",
      "    SystemMessage\n",
      ")\n",
      "chat = ChatOpenAI(temperature = 0)\n",
      "chat([HumanMessage(content = \"Translate this sentence from English to French. It is a beautiful day!\")])\n",
      "chat = ChatOpenAI(temperature = 0, model_name = 'gpt-3.5-turbo')\n",
      "messages = [\n",
      "    SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
      "    HumanMessage(content=\"It is a beautiful day!\")\n",
      "]\n",
      "\n",
      "chat(messages)\n",
      "batch_messages = [\n",
      "    [\n",
      "        SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
      "        HumanMessage(content = \"It is a beautiful day!\")\n",
      "    ],\n",
      "    [\n",
      "        SystemMessage(content = \"You are a helpful assistant that translates English to French\"),\n",
      "        HumanMessage(content = \"Let's go for a hike!\")\n",
      "    ],\n",
      "]\n",
      "\n",
      "result = chat.generate(batch_messages)\n",
      "result\n",
      "result.llm_output['token_usage']\n",
      "# load the templates\n",
      "from langchain.prompts.chat import (\n",
      "    ChatPromptTemplate,\n",
      "    SystemMessagePromptTemplate,\n",
      "    HumanMessagePromptTemplate,\n",
      ")\n",
      "\n",
      "# create the chat\n",
      "\n",
      "chat = ChatOpenAI(temperature = 0)\n",
      "\n",
      "# set the system message prompt template\n",
      "\n",
      "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
      "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
      "\n",
      "# set the human message prompt template\n",
      "\n",
      "human_template = \"{text}\"\n",
      "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
      "\n",
      "# creaet the chat prompt template\n",
      "\n",
      "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
      "\n",
      "# format the prompt using the chat template and input for the templates. Then generate out put.\n",
      "\n",
      "chat(chat_prompt.format_prompt(input_language=\"English\", \n",
      "                               output_language = \"French\", \n",
      "                               text = \"It is a beautiful day!\").to_messages())\n",
      "from langchain import LLMChain\n",
      "from langchain.prompts.chat import (\n",
      "    ChatPromptTemplate,\n",
      "    SystemMessagePromptTemplate,\n",
      "    HumanMessagePromptTemplate,\n",
      "    )\n",
      "\n",
      "chat = ChatOpenAI(temperature = 0)\n",
      "\n",
      "# set the system message prompt template\n",
      "\n",
      "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
      "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
      "\n",
      "# set the human message prompt template\n",
      "\n",
      "human_template = \"{text}\"\n",
      "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
      "\n",
      "# Set the chat prompt template using the system and human prompt template\n",
      "\n",
      "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
      "# create and run the chain using the chat prompt template we just set\n",
      "\n",
      "chain = LLMChain(llm = chat, prompt = chat_prompt)\n",
      "chain.run(input_language = \"English\", output_language = \"French\", text = \"It is a beautiful day!\")\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "# Let's load the language model we're going to use to control the agent.\n",
      "chat = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Now let's load tools to use we plan to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
      "\n",
      "llm = OpenAI(temperature=0)\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
      "\n",
      "\n",
      "# Finally, let's initialize an agent with the tools, the chat model, and the type of agent we want to use.\n",
      "\n",
      "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "\n",
      "# Now let's test it out!\n",
      "agent.run(\"How tall is Niagara Falls? What would be its height if it was two times as tall?\")\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "# Let's load the language model we're going to use to control the agent.\n",
      "chat = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Now let's load tools to use we plan to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
      "\n",
      "llm = OpenAI(temperature=0)\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
      "\n",
      "\n",
      "# Finally, let's initialize an agent with the tools, the chat model, and the type of agent we want to use.\n",
      "\n",
      "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "\n",
      "# Now let's test it out!\n",
      "agent.run(\"How tall is Niagara Falls? What would be its height if it was two times as tall?\")\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "# Let's load the language model we're going to use to control the agent.\n",
      "chat = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Now let's load tools to use we plan to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
      "\n",
      "llm = OpenAI(temperature=0)\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
      "\n",
      "\n",
      "# Finally, let's initialize an agent with the tools, the chat model, and the type of agent we want to use.\n",
      "\n",
      "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "\n",
      "# Now let's test it out!\n",
      "agent.run(\"How tall is Niagara Falls? What would be its height if it was two times as tall?\")\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.agents import load_tools\n",
      "from langchain.agents import initialize_agent\n",
      "from langchain.agents import AgentType\n",
      "\n",
      "# Let's load the language model we're going to use to control the agent.\n",
      "chat = ChatOpenAI(temperature=0)\n",
      "\n",
      "# Now let's load tools to use we plan to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
      "\n",
      "llm = OpenAI(temperature=0)\n",
      "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
      "\n",
      "\n",
      "# Finally, let's initialize an agent with the tools, the chat model, and the type of agent we want to use.\n",
      "\n",
      "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
      "\n",
      "# Now let's test it out!\n",
      "agent.run(\"How tall is Niagara Falls? What would be its height if it was two times as tall?\")\n",
      "from langchain.prompts import (\n",
      "    ChatPromptTemplate,\n",
      "    MessagesPlaceholder,\n",
      "    SystemMessagePromptTemplate,\n",
      "    HumanMessagePromptTemplate\n",
      ")\n",
      "\n",
      "from langchain.chains import ConversationChain\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
      "            MessagesPlaceholder(variable_name = \"history\"),\n",
      "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
      "        ])\n",
      "# set up the chat and conversation chain\n",
      "\n",
      "chat = ChatOpenAI(temperature = 0)\n",
      "memory = ConversationBufferMemory(return_messages = True)\n",
      "conversation = ConversationChain(memory = memory, prompt = prompt, llm = chat)\n",
      "# predict the next response\n",
      "\n",
      "conversation.predict(input=\"Hello, how are you?\")\n",
      "# Continue the conversation to the next predict\n",
      "\n",
      "conversation.predict(input=\"It is a beautiful day! Going out for a hike would be nice. Good hikes in the Catskills?\")\n",
      "# Continue the conversation to the next predict\n",
      "\n",
      "conversation.predict(input=\"It is a beautiful day! Going out for a hike would be nice. Good hikes in the Catskills?\")\n",
      "# Continue the conversation to the next predict\n",
      "\n",
      "conversation.predict(input=\"Thanks! Tell me about yourself \")\n",
      "memory\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b44e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
